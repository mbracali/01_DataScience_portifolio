{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920bedd5-f1a0-4e9c-9122-e030e098866e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Nutrien + LLMs - Llama2/Vicuna com RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ea604-75d2-42d3-8ce6-0117b8c82ad1",
   "metadata": {},
   "source": [
    "Esse notebook foi criado com o intuito de explorar as possibilidades de utilização de LLM's com a técnica de RAG. Aqui vamos utilizar modelos de conversação para extrair dados de textos completos e responder de forma coesa perguntas inputadas pelo usuário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bdc124-cb50-4939-98ab-3915ada6d73e",
   "metadata": {},
   "source": [
    "<h2>Prefácio e considerações</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a4dfe-b343-461b-87b9-14e2ae1e4bf5",
   "metadata": {},
   "source": [
    "<h3>Glossário + links</h3>\n",
    "\n",
    "* O que é um __LLM__: https://pt.wikipedia.org/wiki/Modelo_de_linguagem_grande \n",
    "* Um LLM chamado __LLAMA2__: https://pt.wikipedia.org/wiki/LLaMA\n",
    "    * __Download__ do LLAMA2 CHAT 7b: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q5_K_M.gguf\n",
    "    * __Download__ do LLAMA2 CHAT 13b: https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/blob/main/llama-2-13b-chat.Q5_K_S.gguf\n",
    "* Um LLM chamado __Vicuna__: https://en.wikipedia.org/wiki/Vicuna_LLM\n",
    "    * __Download__ do Vicuna 13b Q5: https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGUF/blob/main/vicuna-13b-v1.5-16k.Q5_K_S.gguf\n",
    "    * __Download__ do Vicuna 13b Q3: https://huggingface.co/TheBloke/vicuna-13B-v1.5-16K-GGUF/blob/main/vicuna-13b-v1.5-16k.Q3_K_L.gguf\n",
    "* O que é RAG: https://medium.com/blog-do-zouza/rag-retrieval-augmented-generation-8238a20e381d#:~:text=A%20RAG%20%C3%A9%20uma%20t%C3%A9cnica,de%20dados%20adicionais%20sem%20retreinamento\n",
    "* Links utilizados como __auxilio nessa exploração__:\n",
    "    * Utilização de LLM's de forma __local__: https://python.langchain.com/docs/guides/local_llms\n",
    "    * Guia de referencia da LIB __Langchain__: https://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "    * Guia de referencia da LIB __llama-cpp__: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "    * Guia de referencia da LIB __llama-index__: https://docs.llamaindex.ai/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ea687-34b8-493f-81b2-4057bd493a7e",
   "metadata": {},
   "source": [
    "<h3>Requisitos para reproduzir</h3>\n",
    "\n",
    "Para reproduzir esse notebook na sua máquina é imprescindivel que você tenha instalado as seguintes bibliotecas:\n",
    "* __pandas__: Bibilioteca para manipulação de objetos de tabela no python;\n",
    "* __markdown__: Biblioteca para interpretação de strings no formato markdown (vem por padrão instalado no jupyter, garantir apenas o update);\n",
    "* __langchain__: Utilizado para criar cadeias de query e buscas em textos longos;\n",
    "* __llama-cpp__: Utilizado para interpretar e utilizar LLM's baseados no llama de forma local:\n",
    "* __llama-index__: Essa lib tem a função de fazer operações mais complexas utilizando uma LLM:\n",
    "* __gputil__: Utilizado para recuperar dados sistemicos de placa de video;\n",
    "* __torch__: Utilizado para manipular o backend da rede neural;\n",
    "* __unstructured__: Realiza a leitura de arquivos nao estruturados;\n",
    "* __os__, __psutil__ e __plataform__: Libs padrao do Python3, utilizadas para recuperar informacoes de sistema;\n",
    "<br>\n",
    "\n",
    "Para a instalação:\n",
    "* Caso execute no __windows__ ou no __mac_osx__, a lib __llama-cpp__ tem alguns passos adicionais, veja o guia: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "* Linha para instalação com __pip__:\n",
    "    * pip3 install pandas markdown langchain llama-cpp-python llama-index gputil torch unstructured faiss-cpu\n",
    "* Se você estiver __no Windows, talvez precise especificar a versão do llama-cpp-python__:\n",
    "    * pip install llama-cpp-python==0.1.48\n",
    "* __No Linux__, você pode utilizar o banco faz em GPU\n",
    "    * pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923424d-0a2a-49f4-9efb-84b4d0362405",
   "metadata": {},
   "source": [
    "<h3>Hardware utilizado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bfddeb-535c-48c9-aba2-3352599407dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2024-02-20 16:22:36.080207| Hardware report:\n",
      "        \n",
      "    ==============================================\n",
      "     Software:\n",
      "      Python ver:............3.10.6\n",
      "      OS system:.............Windows\n",
      "      OS name:...............nt\n",
      "      OS plataform:..........10\n",
      "      Machine sys:...........AMD64\n",
      "      Machine architecture...('64bit', 'WindowsPE')\n",
      "\n",
      "    ==============================================\n",
      "     CPU:\n",
      "      Total cores:...........6\n",
      "      Logical cores:.........12\n",
      "      CPU max frequency:.....Max Frequency: 3701.00Mhz\n",
      "      CPU min frequency:.....Min Frequency: 0.00Mhz \n",
      "      CPU frequency now:.....Current Frequency: 3701.00Mhz \n",
      "\n",
      "    ==============================================\n",
      "     RAM:\n",
      "      Total RAM:.............47.89GB\n",
      "      RAM avaliable:.........39.49GB\n",
      "      RAM used:..............8.40GB\n",
      "      RAM%:..................17.5\n",
      "      \n",
      "    ==============================================\n",
      "     Storage:\n",
      "      Partition 1:............DISK1 - Device: C:\\\n",
      "      Partition 2:............DISK1 - Device: X:\\\n",
      "      Partition 3:............DISK1 - Device: Z:\\\n",
      "      Python avaliable HDD:...1.82TB\n",
      "      Python free HDD:........1.10TB\n",
      "\n",
      "    ==============================================\n",
      "     GPU:\n",
      "      GPU:....................[<GPUtil.GPUtil.GPU object at 0x00000156CF5FC1F0>]\n",
      "\n",
      "    ==============================================\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from custom_libs.ds_utils import hardware_info\n",
    "hi = hardware_info()\n",
    "hi.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632386fc-cef4-467f-aa84-b8a5d7421d86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78be5d-2586-4733-aff1-4b01f8fd0458",
   "metadata": {},
   "source": [
    "<h2>Execução da LLM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c65ef1-2637-4a91-994c-a7e5820a4b8d",
   "metadata": {},
   "source": [
    "<h3>Imports de libs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f09da29-2fcd-438c-9981-f258e2f72038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports de libs padrao\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Imports de libs especificos para manipulacao de dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Imports de libs de apresentacao\n",
    "from markdown import markdown\n",
    "\n",
    "# Import das libs para execucao de LLM's\n",
    "from langchain.llms import LlamaCpp\n",
    "#from llama_index.schema import TextNode\n",
    "\n",
    "# Import das lins para RAG\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import de libs customizadas locais\n",
    "from custom_libs.ds_utils import suppress_stdout_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800703f-2ede-4a6d-a1da-67572c32156b",
   "metadata": {},
   "source": [
    "<h3>Funções auxiliares</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8206bf15-7372-4a7e-89aa-8314fc5eeabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class suppress_stdout_stderr(object):\n",
    "    \"\"\"Essa classe tem como objetivo suprimir os logs de execucao\n",
    "    da classe cpp\n",
    "\n",
    "    Args:\n",
    "        object: Qualquer objeto que herde cpp\n",
    "    \"\"\"\n",
    "            \n",
    "    import os, sys\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "      self.outnull_file = open(os.devnull, 'w')\n",
    "      self.errnull_file = open(os.devnull, 'w')\n",
    "\n",
    "      self.old_stdout_fileno_undup    = sys.stdout.fileno()\n",
    "      self.old_stderr_fileno_undup    = sys.stderr.fileno()\n",
    "\n",
    "      self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n",
    "      self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n",
    "\n",
    "      self.old_stdout = sys.stdout\n",
    "      self.old_stderr = sys.stderr\n",
    "\n",
    "      os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n",
    "      os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n",
    "\n",
    "      sys.stdout = self.outnull_file        \n",
    "      sys.stderr = self.errnull_file\n",
    "      return self\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        \n",
    "      sys.stdout = self.old_stdout\n",
    "      sys.stderr = self.old_stderr\n",
    "\n",
    "      os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n",
    "      os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n",
    "\n",
    "      os.close ( self.old_stdout_fileno )\n",
    "      os.close ( self.old_stderr_fileno )\n",
    "\n",
    "      self.outnull_file.close()\n",
    "      self.errnull_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71501b04-790a-447c-9121-387d611d46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para recuperar documentos de uma fpasta\n",
    "def get_documents(path_transcripts='./02_transcript_data', log = False):\n",
    "\n",
    "    # Cria objeto de leitura\n",
    "    loader = DirectoryLoader(path_transcripts, glob=\"*.txt\")#, loader_cls=PyPDFLoader, show_progress=False)\n",
    "\n",
    "    # Cria objeto com os arquivos de leitura\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Caso o log esteja ligado mostra os documentos carregados\n",
    "    if log:\n",
    "        print(documents)\n",
    "\n",
    "    # Retorna todos os documentos carregados\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cff1d61-417c-4d4e-8872-274237e600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(documents, path_storage = './00_storage', \n",
    "                      device = 'cpu', log = False):\n",
    "    # Carrega arquivos txt recebendo uma colecao de arquivos txt\n",
    "    len(documents)\n",
    "\n",
    "    # Divide os arquivos txt em chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Carrega modelo de embedding\n",
    "    embedding_function = HuggingFaceEmbeddings(model_kwargs={'device': device})\n",
    "    embedding_function.embed_query(texts[0].page_content)\n",
    "\n",
    "    # Cria e persiste um base FAISS\n",
    "    vector_database = FAISS.from_documents(texts, embedding_function)\n",
    "\n",
    "    # Tenta salvar a vector store no caminho de storage\n",
    "    try:\n",
    "        # Tenta persistir a base de vetores\n",
    "        vector_database.save_local(path_storage)\n",
    "\n",
    "        # Caso o log esteja ligado avisa sobre a persistencia do vetor\n",
    "        if log: \n",
    "            print(\"Vector store created in:\", path_storage)\n",
    "            return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Caso o log esteja ligado avisa sobre o erro encontrado\n",
    "        if log: \n",
    "            print(e)\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873ad358-1a17-43f4-a340-d190678225b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(path_storage = './00_storage', device = 'cpu', \n",
    "                    log = False):\n",
    "\n",
    "    try:\n",
    "        # Carrega o modelo de embeddings\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "    \n",
    "        # Carrega a base FAISS\n",
    "        vectorstore = FAISS.load_local(path_storage, embeddings)\n",
    "\n",
    "        if log:\n",
    "            # Caso log esteja ligado avisa sobre o carregamento com sucesso\n",
    "            print(\"VectorStore carregado a partir de:\"+ path_storage)\n",
    "\n",
    "        # Retorna objeto de vetores previamente carregado no disco\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Caso o log esteja ligado avisa sobre o erro encontrado\n",
    "        if log: \n",
    "            print(e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f71abe0-3b3d-43c5-8b07-708dcc096688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponseText(prompt, vector_store):\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    response_raw_texts = vector_store.similarity_search(prompt, top_k=1)\n",
    "    \n",
    "    for document in response_raw_texts:\n",
    "        response += document.page_content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86917906-df11-4e63-966b-234509d14a52",
   "metadata": {},
   "source": [
    "<h3>Inicialização de variaveis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae4c336-73ee-47d9-94e0-d412d8597a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from ./01_models/llama-2-13b-chat.Q5_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 16\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q5_K:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Small\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 8.36 GiB (5.51 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
      "env: FORCE_CMAKE=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_tensors:        CPU buffer size =  8555.93 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    72.03 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   800.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'llama.feed_forward_length': '13824', 'llama.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '16', 'llama.attention.head_count_kv': '40', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n"
     ]
    }
   ],
   "source": [
    "# Caso esteja no MacOsX, habilita processamento Metal\n",
    "%env CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "\n",
    "# Forca compilacao em C, quando disponivel\n",
    "%env FORCE_CMAKE=1\n",
    "\n",
    "# Objeto que guarda a LLM que vai ser utilizada\n",
    "llm = LlamaCpp(\n",
    "    model_path = \"./01_models/llama-2-13b-chat.Q5_K_S.gguf\", # Melhores respostas\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=2048,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    "    )\n",
    "\n",
    "# Variavel de utilizacao de GPU ou nao\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Variaveis de caminho de pastas \n",
    "path_storage = './00_storage'\n",
    "path_models = './01_models'\n",
    "path_transcripts = './02_transcript_data'\n",
    "path_results = './03_results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f433da-a440-46e5-991d-ef4fffe14a4f",
   "metadata": {},
   "source": [
    "<h3>Criando artificios de apoio</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9fd1f8-9b29-4164-ae5d-af71e4e7ff72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 54s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gera lista de documentos que vao ser inseridos no contexto de resposta da llm\n",
    "documents = get_documents(path_transcripts)\n",
    "\n",
    "# Constroi o banco de dados vetorizado\n",
    "build_vectorstore(documents, path_storage)\n",
    "\n",
    "# Gera objeto da vector store \n",
    "vector_store = get_vectorstore(path_storage)\n",
    "\n",
    "# Devolve o banco em um objeto retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75838b0-0fce-4a5f-b1ae-b6e145dacc1e",
   "metadata": {},
   "source": [
    "<h2>Cria template de resposta para perguntas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031fb93a-60a2-465e-a1a6-eea198f8ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o template de resposta\n",
    "prompt_template= \"\"\"\n",
    "### [INST] \n",
    "Instructions: Answer in portuguese, and take the following context in mind:\n",
    "\n",
    "{context}\n",
    "\n",
    "### Question to answer:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    " \n",
    "# Abstraction of Prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Criando a cadeia LLM\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Cadeia de resposta RAG\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1717ee-90c3-4ec3-b266-f200a0dd05f6",
   "metadata": {},
   "source": [
    "<h2>Teste da LLM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7ef2e59-3df5-483e-a7b0-862915d293fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANNATE é um produto químico utilizado para controlar pragas em culturas. É um insecticida que tem ação seletiva nas culturas recomendadas e deve ser utilizado de acordo com as recomendações da bula. O uso de LANNATE deve ser exclusivo e rotineiro com outros produtos de mecanismo de ação efetivos para a praga alvo. É importante seguir as instruções da bula para evitar danos aos cultivos e ao meio ambiente."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   45029.52 ms\n",
      "llama_print_timings:      sample time =      16.99 ms /   122 runs   (    0.14 ms per token,  7181.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45029.14 ms /   315 tokens (  142.95 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:        eval time =   32686.83 ms /   121 runs   (  270.14 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =   78102.34 ms /   436 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 5s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Teste com contexto\n",
    "output_with_rag = rag_chain.invoke(\"O que é LANNATE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a29f8b-3696-4732-9092-95a3c10ba9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número de registro MAPA do LANNATE é 16812."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   45029.52 ms\n",
      "llama_print_timings:      sample time =       2.93 ms /    22 runs   (    0.13 ms per token,  7500.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39269.11 ms /   248 tokens (  158.34 ms per token,     6.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5743.36 ms /    21 runs   (  273.49 ms per token,     3.66 tokens per second)\n",
      "llama_print_timings:       total time =   45079.78 ms /   269 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 18s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Teste com contexto\n",
    "output_with_rag = rag_chain.invoke(\"Qual é o numero mapa do LANNATE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42682186-0820-45bf-894d-aa226a6b0624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Com base nas informações fornecidas, a dose recomendada de LANNATE é de 0,6 L/ha ou 129 g i.a./ha. Isso é citado nas informações da bula, especificamente na parte que diz: \"LANNATE® BR aplicado a partir da dose 0,6 L/ha ou 129 g i.a./ha, apresenta ação seletiva para este alvo nesta cultura.\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   45029.52 ms\n",
      "llama_print_timings:      sample time =      14.42 ms /   115 runs   (    0.13 ms per token,  7976.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34464.61 ms /   245 tokens (  140.67 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time =   27899.96 ms /   114 runs   (  244.74 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:       total time =   62690.50 ms /   359 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 53s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Teste com contexto\n",
    "output_with_rag = rag_chain.invoke(\"Qual é a dose recomendada de LANNATE?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b0bb7-f24b-46ae-bd2e-3ec688476a7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46430b0a-272c-42cf-bfc7-f95787af0403",
   "metadata": {},
   "source": [
    "<h2>Pip Freeze</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69439b0b-44e3-4e55-8f23-dd16628861fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about-time==3.1.1\n",
      "aiohttp==3.9.3\n",
      "aiosignal==1.3.1\n",
      "alive-progress==2.4.1\n",
      "annotated-types==0.6.0\n",
      "anyio==3.6.1\n",
      "appdirs==1.4.4\n",
      "argcomplete==3.1.6\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "astropy==5.1.1\n",
      "asttokens==2.0.8\n",
      "async-generator==1.10\n",
      "async-timeout==4.0.3\n",
      "attrs==22.1.0\n",
      "autoslot==2021.10.1\n",
      "Babel==2.10.3\n",
      "backcall==0.2.0\n",
      "backoff==2.2.1\n",
      "beautifulsoup4==4.12.3\n",
      "beautifultable==1.1.0\n",
      "bleach==5.0.1\n",
      "bs4==0.0.2\n",
      "certifi==2024.2.2\n",
      "cffi==1.15.1\n",
      "chardet==5.0.0\n",
      "charset-normalizer==3.3.2\n",
      "click==8.1.7\n",
      "cmdstanpy==1.0.8\n",
      "colorama==0.4.5\n",
      "commonmark==0.9.1\n",
      "convertdate==2.4.0\n",
      "cryptography==38.0.4\n",
      "cycler==0.11.0\n",
      "dataclasses-json==0.6.4\n",
      "dataclasses-json-speakeasy==0.5.11\n",
      "debugpy==1.6.3\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.14\n",
      "dirtyjson==1.0.8\n",
      "diskcache==5.6.3\n",
      "distro==1.9.0\n",
      "emoji==2.10.1\n",
      "entrypoints==0.4\n",
      "ephem==4.1.3\n",
      "et-xmlfile==1.1.0\n",
      "executing==0.10.0\n",
      "faiss-cpu==1.7.4\n",
      "fastjsonschema==2.16.1\n",
      "filelock==3.13.1\n",
      "filetype==1.2.0\n",
      "fonttools==4.36.0\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "geographiclib==2.0\n",
      "geopy==2.3.0\n",
      "GPUtil==1.4.0\n",
      "grapheme==0.6.0\n",
      "greenlet==3.0.3\n",
      "h11==0.14.0\n",
      "h2==4.1.0\n",
      "hijri-converter==2.2.4\n",
      "holehe==1.61\n",
      "holidays==0.17.2\n",
      "hpack==4.0.0\n",
      "httpcore==0.16.3\n",
      "httpx==0.23.1\n",
      "huggingface-hub==0.20.3\n",
      "humanize==4.4.0\n",
      "hyperframe==6.0.1\n",
      "idna==3.6\n",
      "ImageHash==4.3.1\n",
      "inflection==0.5.1\n",
      "ipykernel==6.15.1\n",
      "ipython==8.4.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==8.0.1\n",
      "jedi==0.18.1\n",
      "Jinja2==3.1.3\n",
      "joblib==1.1.0\n",
      "json5==0.9.10\n",
      "jsonpatch==1.33\n",
      "jsonpath-python==1.0.6\n",
      "jsonpickle==2.2.0\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.13.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==7.3.4\n",
      "jupyter-console==6.4.4\n",
      "jupyter-core==4.11.1\n",
      "jupyter-server==1.18.1\n",
      "jupyterlab==3.4.5\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab-server==2.15.0\n",
      "jupyterlab-widgets==3.0.2\n",
      "kiwisolver==1.4.4\n",
      "korean-lunar-calendar==0.3.1\n",
      "lab==7.1\n",
      "langchain==0.1.8\n",
      "langchain-community==0.0.21\n",
      "langchain-core==0.1.24\n",
      "langdetect==1.0.9\n",
      "langsmith==0.1.3\n",
      "llama-index==0.10.9\n",
      "llama-index-agent-openai==0.1.3\n",
      "llama-index-core==0.10.8.post1\n",
      "llama-index-embeddings-openai==0.1.4\n",
      "llama-index-indices-managed-llama-cloud==0.1.1\n",
      "llama-index-legacy==0.9.48\n",
      "llama-index-llms-openai==0.1.3\n",
      "llama-index-multi-modal-llms-openai==0.1.1\n",
      "llama-index-program-openai==0.1.2\n",
      "llama-index-question-gen-openai==0.1.1\n",
      "llama-index-readers-file==0.1.3\n",
      "llama-index-readers-llama-parse==0.1.0\n",
      "llama-parse==0.3.3\n",
      "llama_cpp_python==0.2.44\n",
      "llamaindex-py-client==0.1.13\n",
      "lml==0.1.0\n",
      "LunarCalendar==0.0.9\n",
      "lxml==4.9.1\n",
      "Markdown==3.5.2\n",
      "MarkupSafe==2.1.5\n",
      "marshmallow==3.20.2\n",
      "matplotlib==3.5.3\n",
      "matplotlib-inline==0.1.6\n",
      "mistune==0.8.4\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multitasking==0.0.11\n",
      "mypy-extensions==1.0.0\n",
      "nbclassic==0.4.3\n",
      "nbclient==0.6.6\n",
      "nbconvert==6.5.3\n",
      "nbformat==5.4.0\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.2.1\n",
      "nltk==3.8.1\n",
      "notebook==6.4.12\n",
      "notebook-shim==0.1.0\n",
      "numpy==1.26.4\n",
      "openai==1.12.0\n",
      "openpyxl==3.0.10\n",
      "outcome==1.3.0.post0\n",
      "packaging==23.2\n",
      "pandas==1.4.3\n",
      "pandas-datareader==0.10.0\n",
      "pandoc==2.2\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "patsy==0.5.3\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.3.0\n",
      "pipx==1.3.3\n",
      "platformdirs==4.1.0\n",
      "plotly==5.10.0\n",
      "plumbum==1.7.2\n",
      "ply==3.11\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.30\n",
      "prophet==1.1.1\n",
      "protobuf==4.21.9\n",
      "psutil==5.9.1\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "pydantic==2.6.1\n",
      "pydantic_core==2.16.2\n",
      "pyerfa==2.0.0.1\n",
      "pyexcel==0.7.0\n",
      "pyexcel-io==0.6.6\n",
      "Pygments==2.13.0\n",
      "PyMeeus==0.5.12\n",
      "PyMuPDF==1.23.24\n",
      "PyMuPDFb==1.23.22\n",
      "pyparsing==3.0.9\n",
      "pypdf==4.0.2\n",
      "pyrsistent==0.18.1\n",
      "python-dateutil==2.8.2\n",
      "python-iso639==2024.2.7\n",
      "python-magic==0.4.27\n",
      "pytz==2022.2.1\n",
      "PyWavelets==1.5.0\n",
      "pywin32==304\n",
      "pywinpty==2.0.7\n",
      "PyYAML==6.0.1\n",
      "pyzmq==23.2.1\n",
      "qtconsole==5.3.1\n",
      "QtPy==2.2.0\n",
      "rapidfuzz==3.6.1\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "rfc3986==1.5.0\n",
      "rich==12.6.0\n",
      "safetensors==0.4.2\n",
      "scikit-learn==1.1.2\n",
      "scipy==1.12.0\n",
      "seaborn==0.11.2\n",
      "Send2Trash==1.8.0\n",
      "sentence-transformers==2.3.1\n",
      "sentencepiece==0.2.0\n",
      "setuptools-git==1.2\n",
      "simplejson==3.17.6\n",
      "six==1.16.0\n",
      "sniffio==1.2.0\n",
      "sortedcontainers==2.4.0\n",
      "soupsieve==2.3.2.post1\n",
      "SQLAlchemy==2.0.27\n",
      "stack-data==0.4.0\n",
      "statsmodels==0.13.5\n",
      "superset==0.30.1\n",
      "sympy==1.12\n",
      "tabulate==0.9.0\n",
      "tenacity==8.2.3\n",
      "termcolor==2.4.0\n",
      "terminado==0.15.0\n",
      "texttable==1.6.4\n",
      "threadpoolctl==3.1.0\n",
      "tiktoken==0.6.0\n",
      "tinycss2==1.1.1\n",
      "tokenizers==0.15.2\n",
      "tomli==2.0.1\n",
      "torch==2.2.0\n",
      "tornado==6.2\n",
      "tqdm==4.66.2\n",
      "traitlets==5.3.0\n",
      "transformers==4.37.2\n",
      "trio==0.21.0\n",
      "txt2tags==3.7\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.9.0\n",
      "unstructured==0.12.4\n",
      "unstructured-client==0.18.0\n",
      "urllib3==2.2.1\n",
      "userpath==1.9.1\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.3.3\n",
      "widgetsnbextension==4.0.2\n",
      "wrapt==1.16.0\n",
      "xlrd==2.0.1\n",
      "yarl==1.9.4\n",
      "yfinance==0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
