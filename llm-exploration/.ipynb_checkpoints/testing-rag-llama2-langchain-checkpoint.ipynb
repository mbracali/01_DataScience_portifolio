{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920bedd5-f1a0-4e9c-9122-e030e098866e",
   "metadata": {},
   "source": [
    "<h1>Nutrien + LLMs - Llama2/Vicuna com RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ea604-75d2-42d3-8ce6-0117b8c82ad1",
   "metadata": {},
   "source": [
    "Esse notebook foi criado com o intuito de explorar as possibilidades de utilização de LLM's com a técnica de RAG. Aqui vamos utilizar modelos de conversação para extrair dados de textos completos e responder de forma coesa perguntas inputadas pelo usuário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bdc124-cb50-4939-98ab-3915ada6d73e",
   "metadata": {},
   "source": [
    "<h2>Prefácio e considerações</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a4dfe-b343-461b-87b9-14e2ae1e4bf5",
   "metadata": {},
   "source": [
    "<h3>Glossário + links</h3>\n",
    "\n",
    "* O que é um __LLM__: https://pt.wikipedia.org/wiki/Modelo_de_linguagem_grande \n",
    "* Um LLM chamado __LLAMA2__: https://pt.wikipedia.org/wiki/LLaMA\n",
    "    * Download utilziado nessa exploração:\n",
    "* Um LLM chamado __Vicuna__: https://en.wikipedia.org/wiki/Vicuna_LLM\n",
    "    * Download utilziado nessa exploração:\n",
    "* O que é RAG: https://medium.com/blog-do-zouza/rag-retrieval-augmented-generation-8238a20e381d#:~:text=A%20RAG%20%C3%A9%20uma%20t%C3%A9cnica,de%20dados%20adicionais%20sem%20retreinamento\n",
    "* Links utilizados como __auxilio nessa exploração__:\n",
    "    * Utilização de LLM's de forma __local__: https://python.langchain.com/docs/guides/local_llms\n",
    "    * Guia de referencia da LIB __Langchain__: https://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "    * Guia de referencia da LIB __llama-cpp__: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "    * Guia de referencia da LIB __llama-index__: https://docs.llamaindex.ai/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ea687-34b8-493f-81b2-4057bd493a7e",
   "metadata": {},
   "source": [
    "<h3>Requisitos para reproduzir</h3>\n",
    "\n",
    "Para reproduzir esse notebook na sua máquina é imprescindivel que você tenha instalado as seguintes bibliotecas:\n",
    "* __pandas__: Bibilioteca para manipulação de objetos de tabela no python;\n",
    "* __markdown__: Biblioteca para interpretação de strings no formato markdown (vem por padrão instalado no jupyter, garantir apenas o update);\n",
    "* __langchain__: Utilizado para criar cadeias de query e buscas em textos longos;\n",
    "* __llama-cpp__: Utilizado para interpretar e utilizar LLM's baseados no llama de forma local:\n",
    "* __llama-index__: Essa lib tem a função de fazer operações mais complexas utilizando uma LLM:\n",
    "* __gputil__: Utilizado para recuperar dados sistemicos de placa de video;\n",
    "* __torch__: Utilizado para manipular o backend da rede neural;\n",
    "* __os__, __psutil__ e __plataform__: Libs padrao do Python3, utilizadas para recuperar informacoes de sistema;\n",
    "<br>\n",
    "\n",
    "Para a instalação:\n",
    "* Caso execute no __windows__ ou no __mac_osx__, a lib __llama-cpp__ tem alguns passos adicionais, veja o guia: https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "* Linha para instalação com __pip__:\n",
    "    * pip3 install pandas markdown langchain llama-cpp-python llama-index gputil torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923424d-0a2a-49f4-9efb-84b4d0362405",
   "metadata": {},
   "source": [
    "<h3>Hardware utilizado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bfddeb-535c-48c9-aba2-3352599407dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2024-02-20 13:21:02.515960| Hardware report:\n",
      "        \n",
      "    ==============================================\n",
      "     Software:\n",
      "      Python ver:............3.9.6\n",
      "      OS system:.............Darwin\n",
      "      OS name:...............posix\n",
      "      OS plataform:..........23.2.0\n",
      "      Machine sys:...........arm64\n",
      "      Machine architecture...('64bit', '')\n",
      "\n",
      "    ==============================================\n",
      "     CPU:\n",
      "      Total cores:...........8\n",
      "      Logical cores:.........8\n",
      "      CPU max frequency:.....Max Frequency: 3228.00Mhz\n",
      "      CPU min frequency:.....Min Frequency: 600.00Mhz \n",
      "      CPU frequency now:.....Current Frequency: 3228.00Mhz \n",
      "\n",
      "    ==============================================\n",
      "     RAM:\n",
      "      Total RAM:.............16.00GB\n",
      "      RAM avaliable:.........4.86GB\n",
      "      RAM used:..............6.54GB\n",
      "      RAM%:..................69.6\n",
      "      \n",
      "    ==============================================\n",
      "     Storage:\n",
      "      Partition 1:............DISK1 - Device: /dev/disk3s3s1\n",
      "      Partition 2:............DISK1 - Device: /dev/disk3s6\n",
      "      Partition 3:............DISK1 - Device: /dev/disk3s4\n",
      "      Python avaliable HDD:...460.43GB\n",
      "      Python free HDD:........240.43GB\n",
      "\n",
      "    ==============================================\n",
      "     GPU:\n",
      "      GPU:....................[]\n",
      "\n",
      "    ==============================================\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from custom_libs.ds_utils import hardware_info\n",
    "hi = hardware_info()\n",
    "hi.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632386fc-cef4-467f-aa84-b8a5d7421d86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78be5d-2586-4733-aff1-4b01f8fd0458",
   "metadata": {},
   "source": [
    "<h2>Execução da LLM</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c65ef1-2637-4a91-994c-a7e5820a4b8d",
   "metadata": {},
   "source": [
    "<h3>Imports de libs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f09da29-2fcd-438c-9981-f258e2f72038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports de libs padrao\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Imports de libs especificos para manipulacao de dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Imports de libs de apresentacao\n",
    "from markdown import markdown\n",
    "\n",
    "# Import das libs para execucao de LLM's\n",
    "from langchain.llms import LlamaCpp\n",
    "from llama_index.schema import TextNode\n",
    "\n",
    "# Import das lins para RAG\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Import de libs customizadas locais\n",
    "from custom_libs.ds_utils import suppress_stdout_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d800703f-2ede-4a6d-a1da-67572c32156b",
   "metadata": {},
   "source": [
    "<h3>Funções auxiliares</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71501b04-790a-447c-9121-387d611d46f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para recuperar documentos de uma fpasta\n",
    "def get_documents(path_transcripts='./02-transcript-data', log = False):\n",
    "\n",
    "    # Cria objeto de leitura\n",
    "    loader = DirectoryLoader(path_transcripts, glob=\"*.txt\")#, loader_cls=PyPDFLoader, show_progress=False)\n",
    "\n",
    "    # Cria objeto com os arquivos de leitura\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Caso o log esteja ligado mostra os documentos carregados\n",
    "    if log:\n",
    "        print(documents)\n",
    "\n",
    "    # Retorna todos os documentos carregados\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cff1d61-417c-4d4e-8872-274237e600e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(documents, path_storage = './00-storage', \n",
    "                      device = 'cpu', log = False):\n",
    "    # Carrega arquivos txt recebendo uma colecao de arquivos txt\n",
    "    len(documents)\n",
    "\n",
    "    # Divide os arquivos txt em chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Carrega modelo de embedding\n",
    "    embedding_function = HuggingFaceEmbeddings(model_kwargs={'device': device}) #(model_name=path_models+'/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
    "    embedding_function.embed_query(texts[0].page_content)\n",
    "\n",
    "    # Cria e persiste um base FAISS\n",
    "    vector_database = FAISS.from_documents(texts, embedding_function)\n",
    "\n",
    "    # Tenta salvar a vector store no caminho de storage\n",
    "    try:\n",
    "        # Tenta persistir a base de vetores\n",
    "        vector_database.save_local(path_storage)\n",
    "\n",
    "        # Caso o log esteja ligado avisa sobre a persistencia do vetor\n",
    "        if log: \n",
    "            print(\"Vector store created in:\", path_storage)\n",
    "            return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Caso o log esteja ligado avisa sobre o erro encontrado\n",
    "        if log: \n",
    "            print(e)\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873ad358-1a17-43f4-a340-d190678225b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorstore(path_storage = './00-storage', device = 'cpu', \n",
    "                    log = False):\n",
    "\n",
    "    try:\n",
    "        # Carrega o modelo de embeddings\n",
    "        embeddings = HuggingFaceEmbeddings()#(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': device})\n",
    "    \n",
    "        # Carrega a base FAISS\n",
    "        vectorstore = FAISS.load_local(path_storage, embeddings)\n",
    "\n",
    "        if log:\n",
    "            # Caso log esteja ligado avisa sobre o carregamento com sucesso\n",
    "            print(\"VectorStore carregado a partir de:\"+ path_storage)\n",
    "\n",
    "        # Retorna objeto de vetores previamente carregado no disco\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Caso o log esteja ligado avisa sobre o erro encontrado\n",
    "        if log: \n",
    "            print(e)\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f71abe0-3b3d-43c5-8b07-708dcc096688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponseText(prompt, vector_store):\n",
    "\n",
    "    response = \"\"\n",
    "    \n",
    "    response_raw_texts = vector_store.similarity_search(prompt, top_k=1)\n",
    "    \n",
    "    for document in response_raw_texts:\n",
    "        response += document.page_content\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86917906-df11-4e63-966b-234509d14a52",
   "metadata": {},
   "source": [
    "<h3>Inicialização de variaveis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae4c336-73ee-47d9-94e0-d412d8597a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_stdout_stderr():\n",
    "    \n",
    "    # Caso esteja no MacOsX, habilita processamento Metal\n",
    "    %env CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "    \n",
    "    # Forca compilacao em C, quando disponivel\n",
    "    %env FORCE_CMAKE=1\n",
    "\n",
    "    # Objeto que guarda a LLM que vai ser utilizada\n",
    "    llm = LlamaCpp(\n",
    "    #model_path=\"./01-models/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    model_path = \"./01-models/vicuna-13b-v1.5.Q5_K_S.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=1024,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=False,\n",
    "    )\n",
    "\n",
    "    # Variavel de utilizacao de GPU ou nao\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Variaveis de caminho de pastas \n",
    "    path_storage = './00-storage'\n",
    "    path_models = './01-models'\n",
    "    path_transcripts = './02-transcript-data'\n",
    "    path_results = './03-results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f433da-a440-46e5-991d-ef4fffe14a4f",
   "metadata": {},
   "source": [
    "<h3>Criando artificios de apoio</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9fd1f8-9b29-4164-ae5d-af71e4e7ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 37.2 s, total: 1min 42s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gera lista de documentos que vao ser inseridos no contexto de resposta da llm\n",
    "documents = get_documents(path_transcripts)\n",
    "\n",
    "# Constroi o banco de dados vetorizado\n",
    "build_vectorstore(documents, path_storage)\n",
    "\n",
    "# Gera objeto da vector store \n",
    "vector_store = get_vectorstore(path_storage)\n",
    "\n",
    "# Devolve o banco em um objeto retriever\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75838b0-0fce-4a5f-b1ae-b6e145dacc1e",
   "metadata": {},
   "source": [
    "<h2>Cria template de respoosta para perguntas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031fb93a-60a2-465e-a1a6-eea198f8ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o template de resposta\n",
    "prompt_template= \"\"\"\n",
    "### [INST] \n",
    "Instrução: Responda a pergunta baseada no seu conhecimento e leve em consideração o seguinte contexto:\n",
    "\n",
    "{context}\n",
    "\n",
    "### Questão:\n",
    "{question} \n",
    "\n",
    "[/INST]\n",
    "\"\"\"\n",
    " \n",
    "# Abstraction of Prompt\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Criando a cadeia LLM\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Cadeia de resposta RAG\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1717ee-90c3-4ec3-b266-f200a0dd05f6",
   "metadata": {},
   "source": [
    "<h2>Teste da LLM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef2e59-3df5-483e-a7b0-862915d293fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Human**: LAN"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Teste com contexto\n",
    "output_with_rag = rag_chain.invoke(\"O que é LANNATE?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c5a8b-d3a1-446d-8260-5a2e0e895352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a29f8b-3696-4732-9092-95a3c10ba9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42682186-0820-45bf-894d-aa226a6b0624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea77b87-9ba3-466c-8328-64364da6120c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d4bba-99ea-41ae-9a78-9bb210078038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69439b0b-44e3-4e55-8f23-dd16628861fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbd909-7c93-48f3-bdf8-1bd9bf18ac10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa48d9-ecc6-4436-bd15-0b4ad5119b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf2a0a-8361-49e2-b8ca-8f7c97a60f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ce3a3-ff72-4a8f-b2fa-eb52d57634e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234feb1-1f61-472e-9a9d-c12c1e35060f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fde18b-c7d8-43c8-87ed-ff67f8ae6e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9139cd-74d4-4cf0-a560-27067310b583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db59e3-a76d-46ba-b417-d04fe3d48094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
